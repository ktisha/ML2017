\documentclass[a4paper,12pt]{article}
\input{./base}

\begin{document}
  \section*{Вопросы к экзамену}
  
  \subsection*{Основные определения}  
  
  \myparagraph Основные определения в машинном обучении: объект, целевая функция, признак, модель, обучающая выборка, функционал качества, обучение, переобучение. 

  \myparagraph Задачи машинного обучения - обучение с учителем, без учителя. Задачи регрессии и классификации. Задачи снижения размерности и кластеризации. 

  \myparagraph Типы признаков в машинном обучении. Приведите примеры различных признаков.
  
  \myparagraph Определение ROC-кривой.
  
  \subsection*{Метрические классификаторы}  
  
  \myparagraph Метод $k$ ближайших соседей в задаче классификации. 

  \myparagraph Методы отбора признаков. Жадный метод.  

  \myparagraph Определение отступа в метрических алгоритмах классификации. Алгоритм Condensed Nearest Neighbor. 
  
  \myparagraph Метод $k$ ближайших соседей в задаче регрессии.
  
  \myparagraph Обобщение метода $k$ ближайших соседей через взвешенный учет объектов. Ядерная оценка плотности.
  
  \myparagraph Проклятие размерности. Зависимость метода ближайших соседей от масштабирования признаков. Способы стандартизации признаков.
  
  \subsection*{Кластеризация}  
  
  \myparagraph Постановка задачи кластеризации. Цели кластеризации. Типы кластерных структур. Чувствительность к нормировке и масштабированию признаков.
  
  \myparagraph Метод $k$ средних. Особенности метода. 
  
  \myparagraph Степени свободы метода $k$ средних. Метод $k$-means++. Метод Xmeans. 

  \myparagraph Метод распространения близости.
  
  \myparagraph Графовые алгоритмы кластеризации.
  
  \myparagraph Алгоритм Ланса-Уильямса.  

  \subsection*{Деревья принятия решений}  

  \myparagraph Логическая закономерность. Интерпретируемость и информативность.

  \myparagraph Решающий список. Достоинства и недостатки.
  
  \myparagraph Структура решающего дерева, метод спуска по дереву в общем случае. Достоинства и недостатки решающих деревьев.

  \myparagraph Подрезание решающих деревьев.
  
  \myparagraph Небрежные решающие деревья.
  
  \myparagraph Деревья принятия решений в задаче регрессии.
    
  \subsection*{Байесовские методы}
  
  \myparagraph Вероятностная постановка задачи классификации. Функция правдоподобия и априорная вероятность.
  
  \myparagraph Функционал среднего риска. Общая формула байесовского классификатора.
  
  \myparagraph Наивный байесовский классификатор.

  \myparagraph Восстановление плотности распределения по выборке.
  
  \myparagraph Аддитивное сглаживание для байесова классификатора.
  
  \subsection*{Линейные классификаторы}
  
  \myparagraph Модель МакКаллока-Питтса   
  
  \myparagraph Обобщённая модель линейного классификатора. Определение отступа. Минимизация эмпирического риска.

  \myparagraph Метод градиентного спуска. Выбор величины шага.
  
  \myparagraph $L_2$ регуляризация.

  \myparagraph Метод стохастического градиента. Недостатки метода SG и как с ними бороться.  

  \subsection*{Способность к обобщению}
  
  \myparagraph Внутренний и внешний функционал качества. Кросс-валидация. 
  
  \myparagraph Критерий непротиворечивости моделей.

  \myparagraph Аналитическая оценка вероятности переобучения. Схема использования. 
  
  \myparagraph Неравенство Бернштейна-Хёфдинга в применении к задаче выбора модели.
  
  \myparagraph Дихотомии. Функция роста. Точка разрыва.
  
  \myparagraph Оценка на максимальное число дихотомий.
  
  \subsection*{Нейронные сети}
  
  \myparagraph Представимость функций в виде нейросети.
  
  \myparagraph Метод обратного распространения ошибок. Основные недостатки и способы их устранения.

  \myparagraph Выбор начального приближения в градиентных методах настройки нейронных сетей. Функции активации.

  \myparagraph Устройство свёрточной нейросети.
  
  \myparagraph Нейронные сети для задачи регрессии.
  
  \subsection*{Метод опорных векторов}
  
  \myparagraph Постановка задачи SVM. 
  
  \myparagraph Регуляризация в задаче SVM. 
  
  \myparagraph Двойственная задача SVM. 

  \myparagraph Ядерный алгоритм SVM. 
  
  \myparagraph Представление метода опорных векторов в виде нейронной сети.
  
  \myparagraph Метод SVR для задачи регрессии.
      
  \subsection*{Линейная регрессия}
  
  \myparagraph Постановка задачи многомерной линейной регрессии. Матричная запись.
  
  \myparagraph Использование сингулярного разложения для решения задачи наименьших квадратов.

  \myparagraph Проблема «мультиколлинеарности» в задачах многомерной линейной регрессии. 
  
  \myparagraph Гребневая регрессия. Регуляризация Лассо.
  
  \myparagraph Нелинейная регрессия. Метод Ньютона-Гаусса.
  
  \myparagraph Задача уменьшения размерности. Метод главных компонент. 
  
  \subsection*{Анализ смещения и разброса}
  
  \myparagraph Постановка задачи анализа смещения и разброса. Качество обучения в зависимости от пространства моделей.
  
  \myparagraph Внутренний и внешний функционал качества. Средний метод.
  
  \myparagraph Определение смещения и разброса.
  
  \myparagraph Кривые обучения.
  
  \subsection*{Ансамбли}
  
  \myparagraph Определение композиции алгоритмов. Типы композиций.
  
  \myparagraph Взвешенное голосование. Бустинг. Алгоритм AdaBoost. 
  
  \myparagraph Простое голосование. Бэггинг и метод случайных подпространств.
  
  \myparagraph Случайный лес. Стэкинг.  
  
\end{document}
